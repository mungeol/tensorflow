{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "import tensorflow.contrib.layers as lays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_DEFAULTS = [['']] + [[0.0] for i in range(1476)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(inputs):\n",
    "    # encoder\n",
    "    # 28 x 28 x 1   ->  14 x 14 x 32\n",
    "    # 14 x 14 x 32  ->  7 x 7 x 16\n",
    "    # 7 x 7 x 16    ->  7 x 7 x 8\n",
    "    net = lays.conv2d(inputs, 32, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d(net, 16, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d(net, 8, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    # decoder\n",
    "    # 7 x 7 x 8    ->  7 x 7 x 16\n",
    "    # 7 x 7 x 16   ->  14 x 14 x 32\n",
    "    # 14 x 14 x 32  ->  28 x 28 x 1\n",
    "    net = lays.conv2d_transpose(net, 16, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d_transpose(net, 32, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d_transpose(net, 1, [5, 5], stride=1, padding='SAME', activation_fn=tf.nn.tanh)\n",
    "    #print(net.get_shape())\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, mode, params):\n",
    "    ae_inputs = tf.reshape(features['x'], [-1, 6, 246, 1])\n",
    "    ae_outputs = autoencoder(ae_inputs)  # create the Autoencoder network\n",
    "    \n",
    "    # calculate the loss and optimize the network\n",
    "    loss = tf.reduce_mean(tf.square(ae_outputs - ae_inputs))  # claculate the mean square error loss\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        in_out_dist = tf.norm(ae_inputs - ae_outputs, axis=1)\n",
    "        \n",
    "        predictions = {\n",
    "            'score': tf.reduce_sum(in_out_dist, 1),\n",
    "            'hint_index': tf.argmax(in_out_dist, 1),\n",
    "            'pid': features['pid']\n",
    "            #'x': features['x'],\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'predict': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, export_outputs=export_outputs)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "        'mse':tf.metrics.mean_squared_error(ae_inputs, ae_outputs)\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_serving_input_fn():\n",
    "    csv_row = tf.placeholder(\n",
    "        shape=[None],\n",
    "        dtype=tf.string\n",
    "    )\n",
    "    \n",
    "    features = parse_csv(csv_row)\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, {'x': csv_row})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serving_input_fn():\n",
    "    inputs = {}\n",
    "    inputs['x'] = tf.placeholder(shape=[None, 1476], dtype=tf.float32)   \n",
    "    inputs['pid'] = tf.placeholder(shape=[None], dtype=tf.string)\n",
    "      \n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FUNCTIONS = {\n",
    "    'JSON': json_serving_input_fn,\n",
    "    'CSV': csv_serving_input_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(rows_string_tensor):\n",
    "\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "    pid = columns.pop(0)\n",
    "    columns = tf.concat(columns, axis=0)\n",
    "        \n",
    "    return {'x':columns, 'pid':pid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames,\n",
    "                        num_epochs=None,\n",
    "                        shuffle=True,\n",
    "                        skip_header_lines=0,\n",
    "                        batch_size=200):\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    if shuffle:\n",
    "      # Process the files in a random order.\n",
    "      filename_dataset = filename_dataset.shuffle(len(filenames))\n",
    "      \n",
    "    # For each filename, parse it into one element per line, and skip the header\n",
    "    # if necessary.\n",
    "    dataset = filename_dataset.flat_map(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(skip_header_lines))\n",
    "    \n",
    "    dataset = dataset.map(parse_csv)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=batch_size * 10)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features = iterator.get_next()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(run_config, hparams):\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, \n",
    "                                  params=hparams, \n",
    "                                  config=run_config)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hparams):\n",
    "    \"\"\"Run the training and evaluate using the high level API\"\"\"\n",
    "  \n",
    "    train_input = lambda: input_fn(\n",
    "        hparams.train_files,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.train_batch_size\n",
    "    )\n",
    "  \n",
    "    # Don't shuffle evaluation data\n",
    "    eval_input = lambda: input_fn(\n",
    "        hparams.eval_files,\n",
    "        batch_size=hparams.eval_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "  \n",
    "    train_spec = tf.estimator.TrainSpec(train_input,\n",
    "                                        max_steps=hparams.train_steps\n",
    "                                        )\n",
    "  \n",
    "    exporter = tf.estimator.FinalExporter('census',\n",
    "            SERVING_FUNCTIONS[hparams.export_format])\n",
    "    eval_spec = tf.estimator.EvalSpec(eval_input,\n",
    "                                      steps=hparams.eval_steps,\n",
    "                                      exporters=[exporter],\n",
    "                                      name='census-eval'\n",
    "                                      )\n",
    "  \n",
    "    run_config = tf.estimator.RunConfig()\n",
    "    run_config = run_config.replace(model_dir=hparams.job_dir)\n",
    "    \n",
    "    estimator = build_estimator(\n",
    "        run_config=run_config, hparams=hparams\n",
    "    )\n",
    "  \n",
    "    tf.estimator.train_and_evaluate(estimator,\n",
    "                                    train_spec,\n",
    "                                    eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = tf.contrib.training.HParams(\n",
    "    job_dir='output',\n",
    "    train_files=['data/mff_20180506_seg1_train_pid.csv'],\n",
    "    eval_files=['data/mff_20180506_seg1_eval_pid.csv'],\n",
    "    train_steps=1,\n",
    "    eval_steps=1,\n",
    "    export_format='JSON',\n",
    "    #export_format='CSV',\n",
    "    num_epochs=5,\n",
    "    train_batch_size=512,\n",
    "    eval_batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_log_step_count_steps': 100, '_task_id': 0, '_service': None, '_num_ps_replicas': 0, '_master': '', '_tf_random_seed': None, '_keep_checkpoint_max': 5, '_model_dir': 'output', '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_num_worker_replicas': 1, '_is_chief': True, '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7200f7a710>, '_session_config': None, '_save_summary_steps': 100, '_keep_checkpoint_every_n_hours': 10000}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_2/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_2/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into output/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.0648382, step = 1\n",
      "INFO:tensorflow:Loss for final step: 3.0648382.\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_2/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_2/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "INFO:tensorflow:Starting evaluation at 2018-08-07-06:10:04\n",
      "INFO:tensorflow:Restoring parameters from output/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2018-08-07-06:10:05\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 2.7906587, mse = 2.7906587\n",
      "INFO:tensorflow:Performing the final export in the end of training.\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_2/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "[<tf.Tensor 'Conv/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_1/Relu:0' shape=(?, 6, 246, 32) dtype=float32>, <tf.Tensor 'Conv_2/Relu:0' shape=(?, 6, 246, 32) dtype=float32>]\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['predict', 'serving_default']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Restoring parameters from output/model.ckpt-1\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b\"output/export/census/temp-b'1533622205'/saved_model.pb\"\n"
     ]
    }
   ],
   "source": [
    "!rm -rf output\n",
    "run_experiment(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud ml-engine local predict --model-dir=output/export/census/1533192404 --text-instances=data/mff_pred.csv\n",
    "!gcloud ml-engine local predict --model-dir=output/export/census/1533528363 --json-instances=data/mff_pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
