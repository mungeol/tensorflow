{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.learn.python.learn.utils import (\n",
    "    saved_model_export_utils)\n",
    "from tensorflow.contrib.training.python.training import hparam\n",
    "import tensorflow.contrib.layers as lays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_DEFAULTS = [['']] + [[0.0] for i in range(1476)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "def autoencoder(inputs):\n",
    "    # encoder\n",
    "    # 28 x 28 x 1   ->  14 x 14 x 32\n",
    "    # 14 x 14 x 32  ->  7 x 7 x 16\n",
    "    # 7 x 7 x 16    ->  7 x 7 x 8\n",
    "    net = lays.conv2d(inputs, 32, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d(net, 16, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d(net, 8, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    # decoder\n",
    "    # 7 x 7 x 8    ->  7 x 7 x 16\n",
    "    # 7 x 7 x 16   ->  14 x 14 x 32\n",
    "    # 14 x 14 x 32  ->  28 x 28 x 1\n",
    "    net = lays.conv2d_transpose(net, 16, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d_transpose(net, 32, [5, 5], stride=1, padding='SAME')\n",
    "    #print(net.get_shape())\n",
    "    net = lays.conv2d_transpose(net, 1, [5, 5], stride=1, padding='SAME', activation_fn=tf.nn.tanh)\n",
    "    #print(net.get_shape())\n",
    "    return net\n",
    "'''\n",
    "''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder(inputs, filters=[32, 16, 8], kernel=5):\n",
    "    dfilters = filters.copy()\n",
    "    dfilters.reverse()\n",
    "    dfilters.pop(0)\n",
    "    num_lays = len(filters)\n",
    "    \n",
    "    elayers = []\n",
    "    \n",
    "    for i in range(num_lays):\n",
    "        if i == 0:\n",
    "            elayers.append(lays.conv2d(inputs, filters[i], [kernel, kernel], stride=1, padding='SAME'))\n",
    "            print(elayers[i].get_shape())\n",
    "        else:\n",
    "            elayers.append(lays.conv2d(elayers[i - 1], filters[i], [kernel, kernel], stride=1, padding='SAME'))\n",
    "            print(elayers[i].get_shape())\n",
    "        \n",
    "    print('-'*7)\n",
    "    dlayers = []\n",
    "    for i in range(num_lays):\n",
    "        if i == 0:\n",
    "            dlayers.append(lays.conv2d_transpose(elayers[num_lays -1], dfilters[i], [kernel, kernel], stride=1, padding='SAME'))\n",
    "            print(dlayers[i].get_shape())\n",
    "        elif i == num_lays - 1:\n",
    "            dlayers.append(lays.conv2d_transpose(dlayers[i - 1], 1, [kernel, kernel], stride=1, padding='SAME', activation_fn=tf.nn.tanh))\n",
    "            print(dlayers[i].get_shape())\n",
    "        else:\n",
    "            dlayers.append(lays.conv2d_transpose(dlayers[i - 1], dfilters[i], [kernel, kernel], stride=1, padding='SAME'))\n",
    "            print(dlayers[i].get_shape())\n",
    "    \n",
    "    return dlayers[num_lays - 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, mode, params):\n",
    "    ae_inputs = tf.reshape(features['x'], [-1, 6, 246, 1])\n",
    "    ae_outputs = autoencoder(ae_inputs, ast.literal_eval(params.filters), params.kernel)  # create the Autoencoder network\n",
    "    \n",
    "    # calculate the loss and optimize the network\n",
    "    loss = tf.reduce_mean(tf.square(ae_outputs - ae_inputs))  # claculate the mean square error loss\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        in_out_dist = tf.norm(ae_inputs - ae_outputs, axis=1)\n",
    "        \n",
    "        predictions = {\n",
    "            'score': tf.squeeze(tf.reduce_sum(in_out_dist, 1)),\n",
    "            'hint_index': tf.squeeze(tf.argmax(in_out_dist, 1)),\n",
    "            'pid': features['pid']\n",
    "            #'x': features['x'],\n",
    "        }\n",
    "        export_outputs = {\n",
    "            'predict': tf.estimator.export.PredictOutput(predictions)\n",
    "        }\n",
    "        \n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions, export_outputs=export_outputs)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        train_op = tf.train.AdamOptimizer(learning_rate=params.learning_rate).minimize(\n",
    "            loss, global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    eval_metric_ops = {\n",
    "        'mse':tf.metrics.mean_squared_error(ae_inputs, ae_outputs)\n",
    "    }\n",
    "    \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_serving_input_fn():\n",
    "    csv_row = tf.placeholder(\n",
    "        shape=[None],\n",
    "        dtype=tf.string\n",
    "    )\n",
    "    \n",
    "    features = parse_csv(csv_row)\n",
    "    \n",
    "    return tf.estimator.export.ServingInputReceiver(features, {'x': csv_row})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_serving_input_fn():\n",
    "    inputs = {}\n",
    "    inputs['x'] = tf.placeholder(shape=[None, 1476], dtype=tf.float32)   \n",
    "    inputs['pid'] = tf.placeholder(shape=[None], dtype=tf.string)\n",
    "      \n",
    "    return tf.estimator.export.ServingInputReceiver(inputs, inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_FUNCTIONS = {\n",
    "    'JSON': json_serving_input_fn,\n",
    "    'CSV': csv_serving_input_fn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(rows_string_tensor):\n",
    "\n",
    "    row_columns = tf.expand_dims(rows_string_tensor, -1)\n",
    "    columns = tf.decode_csv(row_columns, record_defaults=CSV_COLUMN_DEFAULTS)\n",
    "    pid = columns.pop(0)\n",
    "    columns = tf.concat(columns, axis=0)\n",
    "        \n",
    "    return {'x':columns, 'pid':pid}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_fn(filenames,\n",
    "                        num_epochs=None,\n",
    "                        shuffle=True,\n",
    "                        skip_header_lines=0,\n",
    "                        batch_size=200):\n",
    "    filename_dataset = tf.data.Dataset.from_tensor_slices(filenames)\n",
    "    if shuffle:\n",
    "      # Process the files in a random order.\n",
    "      filename_dataset = filename_dataset.shuffle(len(filenames))\n",
    "      \n",
    "    # For each filename, parse it into one element per line, and skip the header\n",
    "    # if necessary.\n",
    "    dataset = filename_dataset.flat_map(\n",
    "        lambda filename: tf.data.TextLineDataset(filename).skip(skip_header_lines))\n",
    "    \n",
    "    dataset = dataset.map(parse_csv)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=batch_size * 10)\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features = iterator.get_next()\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_estimator(run_config, hparams):\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, \n",
    "                                  params=hparams, \n",
    "                                  config=run_config)\n",
    "\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(hparams):\n",
    "    \"\"\"Run the training and evaluate using the high level API\"\"\"\n",
    "  \n",
    "    train_input = lambda: input_fn(\n",
    "        hparams.train_files,\n",
    "        num_epochs=hparams.num_epochs,\n",
    "        batch_size=hparams.train_batch_size\n",
    "    )\n",
    "  \n",
    "    # Don't shuffle evaluation data\n",
    "    eval_input = lambda: input_fn(\n",
    "        hparams.eval_files,\n",
    "        batch_size=hparams.eval_batch_size,\n",
    "        shuffle=False\n",
    "    )\n",
    "  \n",
    "    train_spec = tf.estimator.TrainSpec(train_input,\n",
    "                                        max_steps=hparams.train_steps\n",
    "                                        )\n",
    "  \n",
    "    exporter = tf.estimator.FinalExporter('census',\n",
    "            SERVING_FUNCTIONS[hparams.export_format])\n",
    "    eval_spec = tf.estimator.EvalSpec(eval_input,\n",
    "                                      steps=hparams.eval_steps,\n",
    "                                      exporters=[exporter],\n",
    "                                      name='census-eval'\n",
    "                                      )\n",
    "  \n",
    "    run_config = tf.estimator.RunConfig()\n",
    "    run_config = run_config.replace(model_dir=hparams.job_dir)\n",
    "    \n",
    "    estimator = build_estimator(\n",
    "        run_config=run_config, hparams=hparams\n",
    "    )\n",
    "  \n",
    "    tf.estimator.train_and_evaluate(estimator,\n",
    "                                    train_spec,\n",
    "                                    eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = tf.contrib.training.HParams(\n",
    "    job_dir='output',\n",
    "    train_files=['data/mff_20180506_seg1_train_pid.csv'],\n",
    "    eval_files=['data/mff_20180506_seg1_eval_pid.csv'],\n",
    "    train_steps=1,\n",
    "    eval_steps=1,\n",
    "    export_format='JSON',\n",
    "    #export_format='CSV',\n",
    "    num_epochs=5,\n",
    "    train_batch_size=512,\n",
    "    eval_batch_size=32,\n",
    "    filters='[32, 16, 8]', \n",
    "    kernel=5,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_is_chief': True, '_save_checkpoints_secs': 600, '_tf_random_seed': None, '_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_task_id': 0, '_model_dir': 'output', '_task_type': 'worker', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fb3508dbc50>, '_session_config': None, '_keep_checkpoint_every_n_hours': 10000, '_save_summary_steps': 100, '_num_worker_replicas': 1, '_service': None, '_master': '', '_keep_checkpoint_max': 5, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "(?, 6, 246, 32)\n",
      "(?, 6, 246, 16)\n",
      "(?, 6, 246, 8)\n",
      "-------\n",
      "(?, 6, 246, 16)\n",
      "(?, 6, 246, 32)\n",
      "(?, 6, 246, 1)\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n"
     ]
    }
   ],
   "source": [
    "!rm -rf output\n",
    "run_experiment(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!gcloud ml-engine local predict --model-dir=output/export/census/1533192404 --text-instances=data/mff_pred.csv\n",
    "!gcloud ml-engine local predict --model-dir=output/export/census/1533528363 --json-instances=data/mff_pred.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
