{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESERIES_COL = 'x'\n",
    "DEFAULTS = [[0.0] for i in range(30)] + [[0]]\n",
    "LIST_OF_LABELS = \"non_fraud,fraud\".split(',')\n",
    "NCLASSES = len(LIST_OF_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(hparams):\n",
    "    global DEFAULTS\n",
    "    DEFAULTS = [[0.0] for i in range(30)] + [[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(features, mode, params):\n",
    "    X = features[TIMESERIES_COL]\n",
    "    ylogits = tf.layers.dense(X, units = NCLASSES, activation = None)\n",
    "    return ylogits, NCLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(filename, mode, batch_size=512, skip_header_lines=0):\n",
    "    def _input_fn():\n",
    "        def decode_csv(row):\n",
    "            # row is a string tensor containing the contents of one row\n",
    "            features = tf.decode_csv(row, record_defaults=DEFAULTS)  # string tensor -> list of 50 rank 0 float tensors\n",
    "            label = features.pop()  # remove last feature and use as label\n",
    "            features = tf.stack(features)  # list of rank 0 tensors -> single rank 1 tensor\n",
    "            return {TIMESERIES_COL: features}, label\n",
    "\n",
    "        # Create list of file names that match \"glob\" pattern (i.e. data_file_*.csv)\n",
    "        dataset = tf.data.Dataset.list_files(filename)\n",
    "        # Read in data from files\n",
    "        dataset = dataset.flat_map(\n",
    "            lambda filename: tf.data.TextLineDataset(filename).skip(skip_header_lines)\n",
    "        )\n",
    "        # Parse text lines as comma-separated values (CSV)\n",
    "        dataset = dataset.map(decode_csv)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            num_epochs = None  # loop indefinitely\n",
    "            dataset = dataset.shuffle(buffer_size=10 * batch_size)\n",
    "        else:\n",
    "            num_epochs = 1  # end-of-input after this\n",
    "\n",
    "        dataset = dataset.repeat(num_epochs).batch(batch_size)\n",
    "        return dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "    feature_placeholders = {\n",
    "        TIMESERIES_COL: tf.placeholder(tf.float32, [None, 30])\n",
    "    }\n",
    "\n",
    "    features = {\n",
    "        key: tf.expand_dims(tensor, -1)\n",
    "        for key, tensor in feature_placeholders.items()\n",
    "    }\n",
    "    features[TIMESERIES_COL] = tf.squeeze(features[TIMESERIES_COL], axis=[2])\n",
    "\n",
    "    return tf.estimator.export.ServingInputReceiver(features, feature_placeholders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_classifier(features, labels, mode, params):\n",
    "    model_functions = {\n",
    "        \"linear\": linear_model,\n",
    "        }\n",
    "    model_function = model_functions[params[\"model\"]] \n",
    "    ylogits, nclasses = model_function(features, mode, params)\n",
    "\n",
    "    probabilities = tf.nn.softmax(logits = ylogits)\n",
    "    class_int = tf.cast(x = tf.argmax(input = ylogits, axis = 1), dtype = tf.uint8)\n",
    "    class_str = tf.gather(params = LIST_OF_LABELS, indices = tf.cast(x = class_int, dtype = tf.int32))\n",
    "  \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        # Convert string label to int\n",
    "        #labels_table = tf.contrib.lookup.index_table_from_tensor(mapping = tf.constant(value = LIST_OF_LABELS, dtype = tf.string))\n",
    "        #labels = labels_table.lookup(keys = labels)\n",
    "\n",
    "        loss = tf.reduce_mean(input_tensor = tf.nn.softmax_cross_entropy_with_logits_v2(logits = ylogits, labels = tf.one_hot(indices = labels, depth = NCLASSES)))\n",
    "        \n",
    "        if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "            # This is needed for batch normalization, but has no effect otherwise\n",
    "            update_ops = tf.get_collection(key = tf.GraphKeys.UPDATE_OPS)\n",
    "            with tf.control_dependencies(control_inputs = update_ops):\n",
    "                train_op = tf.contrib.layers.optimize_loss(\n",
    "                    loss = loss, \n",
    "                    global_step = tf.train.get_global_step(),\n",
    "                    learning_rate = params[\"learning_rate\"],\n",
    "                    optimizer = \"Adam\")\n",
    "            eval_metric_ops = None\n",
    "        else:\n",
    "            train_op = None\n",
    "            eval_metric_ops =  {\"accuracy\": tf.metrics.accuracy(labels = labels, predictions = class_int)}\n",
    "    else:\n",
    "        loss = None\n",
    "        train_op = None\n",
    "        eval_metric_ops = None\n",
    " \n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode = mode,\n",
    "        predictions = {\"probabilities\": probabilities, \n",
    "                       \"classid\": class_int, \n",
    "                       \"class\": class_str},\n",
    "        loss = loss,\n",
    "        train_op = train_op,\n",
    "        eval_metric_ops = eval_metric_ops,\n",
    "        export_outputs = {\"classes\": tf.estimator.export.PredictOutput(\n",
    "            {\"probabilities\": probabilities, \n",
    "             \"classid\": class_int, \n",
    "             \"class\": class_str})}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(output_dir, hparams):\n",
    "    tf.summary.FileWriterCache.clear() # ensure filewriter cache is clear for TensorBoard events file\n",
    "    \n",
    "    EVAL_INTERVAL = 300 # every 5 minutes\n",
    "    \n",
    "    # Instantiate base estimator class for custom model function\n",
    "    estimator = tf.estimator.Estimator(\n",
    "        model_fn = image_classifier,\n",
    "        params = hparams,\n",
    "        config = tf.estimator.RunConfig(\n",
    "            save_checkpoints_secs = EVAL_INTERVAL),\n",
    "            model_dir = output_dir)\n",
    "    \n",
    "    # Set estimator's train_spec to use train_input_fn and train for so many steps\n",
    "    train_spec = tf.estimator.TrainSpec(\n",
    "        input_fn = read_dataset(\n",
    "            hparams['train_data_path'],\n",
    "            tf.estimator.ModeKeys.TRAIN,\n",
    "            hparams['batch_size']),\n",
    "        max_steps = hparams[\"train_steps\"])\n",
    "\n",
    "    # Create exporter that uses serving_input_fn to create saved_model for serving\n",
    "    exporter = tf.estimator.LatestExporter(\n",
    "        name = \"exporter\", \n",
    "        serving_input_receiver_fn = serving_input_fn)\n",
    "\n",
    "    # Set estimator's eval_spec to use eval_input_fn and export saved_model\n",
    "    eval_spec = tf.estimator.EvalSpec(\n",
    "        input_fn = read_dataset(\n",
    "            hparams['eval_data_path'],\n",
    "            tf.estimator.ModeKeys.EVAL,\n",
    "            hparams['batch_size']),\n",
    "        steps = None,\n",
    "        exporters = exporter,\n",
    "        start_delay_secs = EVAL_INTERVAL,\n",
    "        throttle_secs = EVAL_INTERVAL)\n",
    "\n",
    "    # Run train_and_evaluate loop\n",
    "    tf.estimator.train_and_evaluate(\n",
    "        estimator = estimator, \n",
    "        train_spec = train_spec, \n",
    "        eval_spec = eval_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'output_dir':'creditcard_trained',\n",
    "    'train_steps':5,\n",
    "    'learning_rate':0.01,\n",
    "    'batch_size':2,\n",
    "    'model':'linear',\n",
    "    'train_data_path':'data/creditcard_train.csv',\n",
    "    'eval_data_path':'data/creditcard_test.csv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTDIR = './creditcard_trained'\n",
    "shutil.rmtree(OUTDIR, ignore_errors = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_global_id_in_cluster': 0, '_train_distribute': None, '_save_checkpoints_secs': 300, '_model_dir': './creditcard_trained', '_keep_checkpoint_max': 5, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_evaluation_master': '', '_service': None, '_log_step_count_steps': 100, '_experimental_distribute': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f1a60144470>, '_keep_checkpoint_every_n_hours': 10000, '_session_creation_timeout_secs': 7200, '_num_worker_replicas': 1, '_protocol': None, '_task_type': 'worker', '_save_checkpoints_steps': None, '_num_ps_replicas': 0, '_experimental_max_worker_delay_secs': None, '_master': '', '_save_summary_steps': 100, '_device_fn': None, '_is_chief': True, '_eval_distribute': None, '_task_id': 0, '_tf_random_seed': None}\n",
      "INFO:tensorflow:Not using Distribute Coordinator.\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after every checkpoint. Checkpoint frequency is determined based on RunConfig arguments: save_checkpoints_steps None or save_checkpoints_secs 300.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ./creditcard_trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.0, step = 0\n",
      "INFO:tensorflow:Saving checkpoints for 5 into ./creditcard_trained/model.ckpt.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2019-12-09T09:08:28Z\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./creditcard_trained/model.ckpt-5\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2019-12-09-09:09:25\n",
      "INFO:tensorflow:Saving dict for global step 5: accuracy = 0.99827254, global_step = 5, loss = 20.333052\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 5: ./creditcard_trained/model.ckpt-5\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow_core/python/saved_model/signature_def_utils_impl.py:201: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Predict: ['serving_default', 'classes']\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Train: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Regress: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Eval: None\n",
      "INFO:tensorflow:Signatures INCLUDED in export for Classify: None\n",
      "INFO:tensorflow:Restoring parameters from ./creditcard_trained/model.ckpt-5\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: ./creditcard_trained/export/exporter/temp-b'1575882565'/saved_model.pb\n",
      "INFO:tensorflow:Loss for final step: 0.0.\n"
     ]
    }
   ],
   "source": [
    "train_and_evaluate(OUTDIR, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
